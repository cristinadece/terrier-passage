{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import os\n",
    "import xml.etree.ElementTree\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qid docid docno doclen rel positions\n",
    "def loadData():\n",
    "    qid2reldoc = defaultdict(list)\n",
    "    qid2irrdoc = defaultdict(list)\n",
    "    docid2doclen = defaultdict(int)\n",
    "    docid2flatlist = defaultdict(list)\n",
    "    \n",
    "    i = 0 \n",
    "    with open(\"wtall_qrels_pos.clean.txt\") as f:\n",
    "        for l in f:\n",
    "            # split row\n",
    "            fields = l.split(\"\\t\")\n",
    "            \n",
    "            # get the data\n",
    "            qid = int(fields[0])\n",
    "            docid = int(fields[1])\n",
    "            doclen=int(fields[3])\n",
    "            rel=int(fields[4])\n",
    "            flat_list = [item for sublist in ast.literal_eval(fields[5]) for item in sublist]\n",
    "\n",
    "            \n",
    "            # put data in dicts\n",
    "            docid2doclen[docid] = doclen\n",
    "            docid2flatlist[qid, docid] = flat_list\n",
    "            if rel > 0:\n",
    "                qid2reldoc[qid].append(docid)\n",
    "            else:\n",
    "                qid2irrdoc[qid].append(docid)\n",
    "\n",
    "    return  qid2reldoc, qid2irrdoc, docid2doclen, docid2flatlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qid2reldoc, qid2irrdoc, docid2doclen, docid2flatlist = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print qid2irrdoc[1]\n",
    "print qid2reldoc[1]\n",
    "\n",
    "a = set(qid2reldoc[1])\n",
    "b = set(qid2irrdoc[1])\n",
    "a.intersection(b)\n",
    "print len(b)\n",
    "print len(a)\n",
    "print len(a.intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadQidInfo():\n",
    "    qidDict = dict()\n",
    "    for filename in os.listdir(os.curdir):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            e = xml.etree.ElementTree.parse(filename).getroot()\n",
    "            for query in e.findall('topic'):\n",
    "                qidDict[int(query.get(\"number\"))] = tuple([int(query.get(\"number\")), query.get(\"type\"), query[0].text])\n",
    "    return qidDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qidInfo = loadQidInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stats on document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL DOCs with RELEVANCE\n",
    "def plotStats(docid2doclen):\n",
    "    sorted_per_freq = Counter(docid2doclen.values()).most_common()\n",
    "    sorted_per_length = sorted(sorted_per_freq, key=lambda i: i[0])\n",
    "    doc_len, doc_len_freq = zip(*sorted_per_length)\n",
    "    plt.loglog(doc_len, doc_len_freq, 'r.')\n",
    "    plt.xlabel(\"DocLen\")\n",
    "    plt.ylabel(\"Freq\")\n",
    "    \n",
    "    \n",
    "plotStats(docid2doclen)\n",
    "stats.describe(docid2doclen.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RELEVANT DOCS\n",
    "relevantdocs = [item for sublist in qid2reldoc.values() for item in sublist]\n",
    "print len(relevantdocs)\n",
    "print Counter(relevantdocs).most_common(20)\n",
    "relevant_doclen_dict = {docid:docid2doclen[docid] for docid in relevantdocs}\n",
    "\n",
    "print \"---------------------------\"\n",
    "print \"Num Relevant Docs (unique): \", len(relevant_doclen_dict)\n",
    "print \"Stats: \",stats.describe(relevant_doclen_dict.values())\n",
    "print \"Median: \", np.median(relevant_doclen_dict.values())\n",
    "plotStats(relevant_doclen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build OCCURENCE Probability MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Samples irrelevant from the same query ID\n",
    "\n",
    "np.random.seed(6)\n",
    "def sample_irr_for_qidA(qid, qid2reldoc, qid2irrdoc):\n",
    "    num_irr = len(qid2reldoc[qid])\n",
    "    # sample larger than population\n",
    "    if num_irr >= len(qid2irrdoc[qid]): \n",
    "        irr_docids = qid2irrdoc[qid]\n",
    "    else:\n",
    "        irr_docids = random.sample(qid2irrdoc[qid],num_irr)\n",
    "    return irr_docids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeProbabDistribution(flatlists, max_doc_len):\n",
    "    occurence_ndarray = np.zeros(max_doc_len)\n",
    "    for flatlist in flatlists:\n",
    "        filtered_flatlist = [x for x in flatlist if x < max_doc_len]\n",
    "        occurence_ndarray[filtered_flatlist] += 1\n",
    "    occurence_ndarray = occurence_ndarray/len(flatlists)\n",
    "    return occurence_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qid_RelevantDocsDistrib = dict()\n",
    "\n",
    "# A. only irrelevant for that query\n",
    "qid_IrrelevantDocDistribA = dict() \n",
    "\n",
    "# ALL\n",
    "qid_allDocsDistrib = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at PER QUERY level\n",
    "\n",
    "#### RELEVANT\n",
    "for qid, relDocidList in qid2reldoc.iteritems():\n",
    "    flatlists = [docid2flatlist[qid, docid] for docid in relDocidList]\n",
    "    occ_probab_distrib = computeProbabDistribution(flatlists, 2000)\n",
    "    if True in np.isnan(occ_probab_distrib):\n",
    "        occ_probab_distrib = np.zeros(2000)\n",
    "    qid_RelevantDocsDistrib[qid] = occ_probab_distrib\n",
    "    \n",
    "print len(qid_RelevantDocsDistrib)\n",
    "print qid_RelevantDocsDistrib.keys()\n",
    "print qid_RelevantDocsDistrib[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IRELEVANT only irrelevant for that query\n",
    "for qid, irelDocidList in qid2irrdoc.iteritems():\n",
    "    if qid in qid_RelevantDocsDistrib.keys():\n",
    "        #irelDocidList = sample_irr_for_qidA(qid, qid2reldoc, qid2irrdoc)\n",
    "        flatlists = [docid2flatlist[qid, docid] for docid in irelDocidList]\n",
    "        occ_probab_distrib = computeProbabDistribution(flatlists, 2000)\n",
    "        qid_IrrelevantDocDistribA[qid] = occ_probab_distrib\n",
    "\n",
    "print len(qid_IrrelevantDocDistribA)\n",
    "print qid_IrrelevantDocDistribA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ALL\n",
    "for qid, irelDocidList in qid2irrdoc.iteritems():\n",
    "    if qid in qid_RelevantDocsDistrib.keys():\n",
    "        all_docids = list()\n",
    "        all_docids.extend(irelDocidList)\n",
    "        all_docids.extend(qid2reldoc[qid])\n",
    "\n",
    "        flatlists = [docid2flatlist[qid, docid] for docid in all_docids if (qid, docid) in docid2flatlist]\n",
    "        occ_probab_distrib = computeProbabDistribution(flatlists, 2000)\n",
    "        qid_allDocsDistrib[qid] = occ_probab_distrib\n",
    "\n",
    "print len(qid_allDocsDistrib)\n",
    "print qid_allDocsDistrib.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucketandreshape(x, bucket_size):\n",
    "    return x.reshape(x.size // bucket_size, bucket_size).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 KL divergence\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
    "\n",
    "scipy.stats.entropy(pk, qk=None, base=None)\n",
    "\n",
    "Calculate the entropy of a distribution for given probability values.\n",
    "If only probabilities pk are given, the entropy is calculated as S = -sum(pk * log(pk), axis=0).\n",
    "If qk is not None, then compute the Kullback-Leibler divergence S = sum(pk * log(pk / qk), axis=0).\n",
    "\n",
    "This routine will normalize pk and qk if they don’t sum to 1.\n",
    "\n",
    "Parameters:\t\n",
    "\n",
    "    pk : sequence\n",
    "       Defines the (discrete) distribution. pk[i] is the (possibly unnormalized) probability of event i.\n",
    "       \n",
    "    qk : sequence, optional\n",
    "        Sequence against which the relative entropy is computed. Should be in the same format as pk.\n",
    "        \n",
    "    base : float, optional\n",
    "        The logarithmic base to use, defaults to e (natural logarithm).\n",
    "Returns:\t\n",
    "\n",
    "    S : float\n",
    "    The calculated entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "bucket_size = 100\n",
    "\n",
    "kl_Rel_IrrelA = dict()\n",
    "kl_Rel_All = dict()\n",
    "mutual_info = dict() # this in sklearn is done on labels, not prob_distrib\n",
    "for qid in qid_RelevantDocsDistrib.keys():\n",
    "    print \n",
    "    print qid\n",
    "    # no bucketing\n",
    "    a = qid_RelevantDocsDistrib[qid]\n",
    "    b = qid_IrrelevantDocDistribA[qid]\n",
    "    c = qid_allDocsDistrib[qid]\n",
    "    \n",
    "    # with bucketing\n",
    "\n",
    "    a = bucketandreshape(a, bucket_size)\n",
    "    print a\n",
    "    b = bucketandreshape(b, bucket_size)\n",
    "    print b\n",
    "    c = bucketandreshape(c, bucket_size)\n",
    "    print c\n",
    "\n",
    "    \n",
    "    kl_Rel_IrrelA[qid] = stats.entropy(a, b, 2.0)\n",
    "    kl_Rel_All[qid] = stats.entropy(a, c, 2.0)\n",
    "    mutual_info[qid] = metrics.mutual_info_score(a, b)\n",
    "    print kl_Rel_IrrelA[qid], kl_Rel_All[qid], mutual_info[qid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.metrics.mutual_info_score([0,1],[1,0]) \n",
    "https://datascience.stackexchange.com/questions/9262/calculating-kl-divergence-in-python\n",
    "\n",
    "Kullback-Leibler divergence is fragile, unfortunately. On above example it is not well-defined: KL([0,1],[1,0]) causes a division by zero, and tends to infinity. It is also asymmetric.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mutual_information\n",
    "\n",
    "https://math.stackexchange.com/questions/21190/justification-for-infinite-kl-divergence\n",
    "https://en.wikipedia.org/wiki/Talk%3AKullback%E2%80%93Leibler_divergence\n",
    "https://en.wikipedia.org/wiki/Statistical_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Jaccard coef\n",
    "\n",
    "needs to be binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jack_Rel_Irrel = dict()\n",
    "jack_Rel_All = dict()\n",
    "bucket_size = 100\n",
    "\n",
    "for qid in qid_RelevantDocsDistrib.keys():\n",
    "    print \n",
    "    print qid\n",
    "    # no bucketing\n",
    "    a = qid_RelevantDocsDistrib[qid]\n",
    "    b = qid_IrrelevantDocDistribA[qid]\n",
    "    c = qid_allDocsDistrib[qid]\n",
    "    \n",
    "    # with bucketing\n",
    "\n",
    "    a1 = bucketandreshape(a, bucket_size)\n",
    "    a = [0 if i<a1.mean() else 1 for i in a1]\n",
    "    print a\n",
    "    b1 = bucketandreshape(b, bucket_size)\n",
    "    b = [0 if i<b1.mean() else 1 for i in b1]\n",
    "    print b\n",
    "    c1 = bucketandreshape(c, bucket_size)\n",
    "    c = [0 if i<c1.mean() else 1 for i in c1]\n",
    "    print c\n",
    "    \n",
    "    # get the coef!\n",
    "    jack_Rel_Irrel[qid] = metrics.jaccard_similarity_score(a,b)\n",
    "    jack_Rel_All[qid] = metrics.jaccard_similarity_score(a,c)\n",
    "    \n",
    "    print jack_Rel_Irrel[qid], jack_Rel_All[qid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lots of other metrics: http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
    "#### Other measures for statistical difference between 2 probab distrib: https://en.wikipedia.org/wiki/Statistical_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PLOTS with different granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all 2000 positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for qid in qid_RelevantDocsDistrib.keys():\n",
    "    plt.figure(figsize=(16, 3))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    #if qid in qid_RelevantDocsDistribFlat:\n",
    "    a = qid_RelevantDocsDistrib[qid]\n",
    "    a = np.insert(a, 0, np.zeros(40))\n",
    "    plt.imshow(a.reshape((51, 40)), cmap=\"hot\") #, interpolation='sinc')\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    b = qid_IrrelevantDocDistribA[qid]\n",
    "    b = np.insert(b, 0, np.zeros(40))\n",
    "    plt.imshow(b.reshape((51, 40)), cmap=\"hot\") #, interpolation='sinc')\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    \n",
    "    c = qid_allDocsDistrib[qid]\n",
    "    c = np.insert(c, 0, np.zeros(40))\n",
    "    plt.imshow(c.reshape((51, 40)), cmap=\"hot\") #, interpolation='sinc')\n",
    "    \n",
    "\n",
    "    plt.suptitle(str(qidInfo[qid])+ \" \" + str(kl_Rel_IrrelA[qid]) + \" \" + str(kl_Rel_All[qid]))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregated bucket size 20 => 100 positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qid_label = []\n",
    "X = []\n",
    "X_irrel = []\n",
    "bucket_size = 20\n",
    "\n",
    "for qid in qid_RelevantDocsDistrib.keys():\n",
    "    plt.figure(figsize=(16, 3))\n",
    "    \n",
    "    qid_label.append(qid)\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    a = qid_RelevantDocsDistrib[qid]\n",
    "    a = bucketandreshape(a, bucket_size)\n",
    "    X.append(a)\n",
    "    plt.imshow(a.reshape((10, 10)), cmap=\"hot\") #, interpolation='sinc')\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    b = qid_IrrelevantDocDistribA[qid]\n",
    "    b = bucketandreshape(b, bucket_size)\n",
    "    X_irrel.append(b)\n",
    "    plt.imshow(b.reshape((10, 10)), cmap=\"hot\") #, interpolation='sinc')\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    c = qid_allDocsDistrib[qid]\n",
    "    c = bucketandreshape(c, bucket_size)\n",
    "    plt.imshow(c.reshape((10, 10)), cmap=\"hot\") #, interpolation='sinc')\n",
    "    \n",
    "\n",
    "    plt.suptitle(str(qidInfo[qid])+ \" \" + str(jack_Rel_Irrel[qid]) + \" \" + str(jack_Rel_All[qid]))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "X = np.array(X)\n",
    "X_irrel = np.array(X_irrel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 5 random qid , with dimension 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "\n",
    "qid_sample = random.sample(qid2reldoc.keys(),5)\n",
    "plt.figure(figsize=(16, 8))\n",
    "for qid in qid_sample:\n",
    "    a = bucketandreshape(qid_RelevantDocsDistrib[qid], 20)\n",
    "    plt.plot(a, label=qid)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pairwise distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances\n",
    "\n",
    "Valid values for metric are:\n",
    "From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]. These metrics support sparse matrix inputs.\n",
    "From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print len(X), len(X[1])\n",
    "print len(X_irrel), len(X_irrel[1])\n",
    "new_X = np.concatenate([X, X_irrel], axis = 1)\n",
    "\n",
    "for metric in ['euclidean','cosine']:\n",
    "    D = metrics.pairwise_distances(X,  metric=metric) \n",
    "    print metric\n",
    "    print D\n",
    "    print \"------------------------\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(D) # interpolation='nearest', cmap=plt.cm.gnuplot2, vmin=0\n",
    "    plt.title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the bigger the value the more different they are\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print len(X), len(X[1])\n",
    "print len(X_irrel), len(X_irrel[1])\n",
    "print len(qid_label)\n",
    "\n",
    "for metric in ['euclidean','cosine']:\n",
    "    D = metrics.pairwise_distances(X, X_irrel,  metric=metric) \n",
    "    print metric\n",
    "#     print D\n",
    "    print \"------------------------\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(D) # interpolation='nearest', cmap=plt.cm.gnuplot2, vmin=0\n",
    "    plt.title(metric)\n",
    "    \n",
    "    for i in range(D.shape[0]):\n",
    "        print D[i,i], qidInfo[qid_label[i]]\n",
    "        \n",
    "#     print D[135,135] , qidInfo[qid_label[135]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print len(X), len(X[1])\n",
    "\n",
    "for metric in ['euclidean', 'cosine']:\n",
    "    D = metrics.pairwise_distances(X,  metric=metric) \n",
    "    print metric\n",
    "    print D\n",
    "    print \"------------------------\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(D) # interpolation='nearest', cmap=plt.cm.gnuplot2, vmin=0\n",
    "    plt.title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.copy()\n",
    "# D[5,98]\n",
    "# D[35, 108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for t in thresholds:\n",
    "    graph = D.copy()\n",
    "    graph = 1 - graph\n",
    "    graph[graph<t] = 0.0\n",
    "    print graph.shape, \"threshold: \", t\n",
    "    n_comp, labels = connected_components(graph, directed=False, connection='weak', return_labels=True)\n",
    "    print n_comp\n",
    "#     print labels\n",
    "    print Counter(labels).most_common()\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### EQUIVALENT TO prev cell\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for t in thresholds[6:10]:\n",
    "    graph = D.copy()\n",
    "    graph = 1 - graph # now we have similarity instead of distance\n",
    "    graph[graph<t] = 0.0\n",
    "    \n",
    "    print\n",
    "    print\n",
    "    print graph.sum()\n",
    "    print graph.shape, \"threshold: \", t\n",
    "\n",
    "    G = nx.from_numpy_matrix(graph)\n",
    "    cc = nx.connected_components(G)\n",
    "#     cc = list(nx.connected_component_subgraphs(G))\n",
    "    print \"Number of connected components\", nx.number_connected_components(G)\n",
    "\n",
    "    for g in cc:\n",
    "        print g\n",
    "\n",
    "    \n",
    "# # https://networkx.github.io/documentation/networkx-1.9.1/reference/algorithms.component.html\n",
    "\n",
    "\n",
    "# nx.draw_random(g)\n",
    "# nx.draw_circular(g)\n",
    "# nx.draw_spectral(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
