{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/multiclass.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "https://stats.stackexchange.com/questions/260754/better-performace-using-random-forest-one-vs-all-than-random-forest-multiclass\n",
    "https://github.com/scikit-learn/scikit-learn/issues/9602\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "http://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. term idf\n",
    "    2. unigram frequency - query in OR\n",
    "    3. bigram frequency - the query in AND\n",
    "    4. query length\n",
    "    5. number of stopwords\n",
    "    -6. query type: faceted, ambiguous-\n",
    "    7. is anchor ???\n",
    "    8. proximity information from matteo's file - proximity models (e.g., [Peng et al. 2007])\n",
    "        that act on pairs of query terms\n",
    "    9. link analysis approaches that consider the linkage patterns within the top-ranked\n",
    "        documents (e.g., SALSA [Lempel and Moran 2001])\n",
    "    10. features from Wikipedia title articles\n",
    "    11. google n-grams\n",
    "    12. query freq in query log, on terms also\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    data = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "    qid = int(data[0])\n",
    "    df_list = json.loads(data[2])\n",
    "    docid = int(data[4])\n",
    "    docno = data[5]\n",
    "    doclen = int(data[6])\n",
    "    tf_list = np.array(json.loads(data[7]))\n",
    "    return df_list, tf_list, doclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTermFeatures(qid_str):\n",
    "    directory = \"/home/muntean/terrier-passage/tfs-per-qid/all-matches-top-10000/\"\n",
    "    filename = directory + \"all-matches-fields-tfs-qid-\" + qid_str + \"-filtered.txt\"\n",
    "    countLines = 0\n",
    "    with open(filename, \"r\") as inputFile:    \n",
    "        df_list, sum_tf_list, sum_doclen = parseLine(inputFile.readline())\n",
    "        countLines += 1\n",
    "        #print(df_list, sum_tf_list, sum_doclen)\n",
    "        for line in inputFile:\n",
    "            countLines += 1\n",
    "            df_list, tf_list, doclen = parseLine(line)\n",
    "            #print(df_list, tf_list, doclen)\n",
    "            sum_tf_list = sum_tf_list + tf_list\n",
    "            sum_doclen = sum_doclen + doclen\n",
    "  \n",
    "        avg_tf_list =  sum_tf_list / countLines\n",
    "        avg_doclen = sum_doclen / countLines\n",
    "        \n",
    "    return df_list, np.sum(avg_tf_list, axis=0), avg_doclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1036360, 1688512, 9909812],\n",
       " array([  1.8164,   1.5978,   5.1386,   4.2499,   7.1304,  32.1869]),\n",
       " 1730.221)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTermFeatures(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf(N, df):\n",
    "    return math.log((N - df + 0.5) / (df + 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET them features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have 2 query types: faceted, ambiguous\n",
    "\"\"\"\n",
    "N = 50220423\n",
    "AVG_LEN = 963.90334\n",
    "\n",
    "def getQueryFeatures(info):\n",
    "    featureDict=defaultdict(float)\n",
    "    \n",
    "    # Query Type\n",
    "    if info[1] == \"faceted\":\n",
    "        featureDict[\"faceted\"]=1\n",
    "        featureDict[\"ambiguous\"]=0\n",
    "    else:\n",
    "        featureDict[\"faceted\"]=0\n",
    "        featureDict[\"ambiguous\"]=1\n",
    "        \n",
    "    # Query len\n",
    "    featureDict[\"qlen\"] = len(info[2].split(\" \"))\n",
    "    \n",
    "    # Stopwords\n",
    "    featureDict[\"num_stopwords\"] = len([x for x in info[2].split(\" \") if x in ENGLISH_STOP_WORDS])\n",
    "    \n",
    "    # Term info\n",
    "    df_list, tf_list_avg, doclen_avg  = getTermFeatures(str(info[0]))\n",
    "    \n",
    "    featureDict[\"maxDF\"] = max(df_list)\n",
    "    featureDict[\"minDF\"] = min(df_list)\n",
    "    featureDict[\"avgDF\"] = np.mean(df_list)\n",
    "    \n",
    "    featureDict[\"maxIDF\"] = max([idf(N, df) for df in df_list])\n",
    "    featureDict[\"minIDF\"] = min([idf(N, df) for df in df_list])\n",
    "    featureDict[\"avgIDF\"] = np.mean([idf(N, df) for df in df_list])\n",
    "    \n",
    "    for i,tf_avg in enumerate(tf_list_avg):\n",
    "        featureDict[\"avgTF\"+str(i+1)] = tf_avg\n",
    "    featureDict[\"avgDoclen\"] = doclen_avg\n",
    "    \n",
    "    deltaTF = [t - s for s, t in zip(tf_list_avg, tf_list_avg[1:])]\n",
    "    for i, delta in enumerate(deltaTF):\n",
    "        featureDict[\"deltaTF\"+str(i+1)] = delta\n",
    "\n",
    "    return featureDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r resultListAltered\n",
    "%store -r resultDict\n",
    "%store -r qidInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverseMappingClusterQid(resultDict):\n",
    "    queryClassDict = {}\n",
    "    for key, values in resultDict.items():\n",
    "        for value in values:\n",
    "            queryClassDict[value]=(key)\n",
    "    return queryClassDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getXy(qidInfo, queryClassDict):\n",
    "    X_as_list = list()\n",
    "    y = list()\n",
    "    for qid, info in qidInfo.items():\n",
    "        if qid in queryClassDict:\n",
    "            featureDict = getQueryFeatures(info)\n",
    "            X_as_list.append(list(featureDict.values()))\n",
    "            queryClass = queryClassDict[qid]\n",
    "            y.append(queryClass)\n",
    "\n",
    "    feature_name = list(featureDict.keys())\n",
    "\n",
    "    # X is the matrix\n",
    "    # y are the classes\n",
    "    X = np.array(X_as_list)\n",
    "    y = np.array(y)\n",
    "    return X,y, feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryClassDict = reverseMappingClusterQid(resultListAltered[0])\n",
    "X,y, feature_name = getXy(qidInfo, queryClassDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 179), (0, 15)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cluster distribution\n",
    "Counter(y).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OneVsRestClassifier - LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muntean/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "clf1 = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y)\n",
    "print(clf1)\n",
    "print(clf1.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "0.927835051546\n",
      "[(0.16608284018470368, 'avgDoclen'), (0.15477483350888507, 'deltaTF2'), (0.13366828351118978, 'qlen'), (0.11543202102715948, 'minDF'), (0.10685713482858081, 'avgDF'), (0.1008059292759466, 'avgTF6'), (0.075220897839400946, 'avgTF3'), (0.042207901366331027, 'maxIDF'), (0.040712645268819077, 'avgTF1'), (0.021937965070843772, 'avgTF5'), (0.021505948496861729, 'deltaTF4'), (0.016491715966748415, 'deltaTF3'), (0.0043018836545296217, 'minIDF'), (0.0, 'num_stopwords'), (0.0, 'maxDF'), (0.0, 'faceted'), (0.0, 'deltaTF5'), (0.0, 'deltaTF1'), (0.0, 'avgTF4'), (0.0, 'avgTF2'), (0.0, 'avgIDF'), (0.0, 'ambiguous')]\n"
     ]
    }
   ],
   "source": [
    "## Random forest classifier - inherently multiclass (not one vs all!)\n",
    "#### http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf2 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "print(clf2.fit(X, y))\n",
    "print(clf2.score(X,y))\n",
    "print(sorted([(x,y) for x,y in zip(clf2.feature_importances_, feature_name)], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OneVsRestClassifier - Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=1.0, loss='deviance', max_depth=1,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
      "              warm_start=False),\n",
      "          n_jobs=1)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "## Gradient Boosting classifier - one vs all\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf3 = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0))\n",
    "print(clf3.fit(X, y))\n",
    "print(clf3.score(X,y))\n",
    "# print([(x,y) for x,y in zip(clf3.feature_importances_, feature_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train multiple classifiers -> soft voting\n",
    "# http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 1\n",
      "[ 0.94871795  0.92307692  0.92307692  0.76923077  0.86842105]\n",
      "Accuracy: 0.89 (+/- 0.13)\n",
      "[ 0.92307692  0.92307692  0.12820513  0.8974359   0.86842105]\n",
      "Accuracy: 0.75 (+/- 0.62)\n",
      "--------------\n",
      "Classifier 2\n",
      "[ 1.          0.97435897  0.94871795  0.76923077  0.92105263]\n",
      "Accuracy: 0.92 (+/- 0.16)\n",
      "[ 0.92307692  0.8974359   0.92307692  0.92307692  0.92105263]\n",
      "Accuracy: 0.92 (+/- 0.02)\n",
      "--------------\n",
      "Classifier 3\n",
      "[ 1.          0.94871795  0.8974359   0.76923077  0.89473684]\n",
      "Accuracy: 0.90 (+/- 0.15)\n",
      "[ 0.8974359   0.92307692  0.92307692  0.84615385  0.89473684]\n",
      "Accuracy: 0.90 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, LeaveOneOut\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "strat_k_fold = StratifiedKFold(n_splits=5)\n",
    "# for train_indices, test_indices in k_fold.split(X):\n",
    "#     print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "\n",
    "\n",
    "print(\"Classifier 1\")\n",
    "a = np.array([clf1.fit(X[train], y[train]).score(X[test], y[test]) for train, test in k_fold.split(X)])\n",
    "print(a)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (a.mean(), a.std() * 2))\n",
    "\n",
    "b = np.array([clf1.fit(X[train], y[train]).score(X[test], y[test]) for train, test in strat_k_fold.split(X,y)])\n",
    "print(b)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (b.mean(), b.std() * 2))\n",
    "print(\"--------------\")\n",
    "\n",
    "\n",
    "print(\"Classifier 2\")\n",
    "a = np.array([clf2.fit(X[train], y[train]).score(X[test], y[test]) for train, test in k_fold.split(X)])\n",
    "print(a)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (a.mean(), a.std() * 2))\n",
    "\n",
    "b = np.array([clf2.fit(X[train], y[train]).score(X[test], y[test]) for train, test in strat_k_fold.split(X,y)])\n",
    "print(b)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (b.mean(), b.std() * 2))\n",
    "print(\"--------------\")\n",
    "\n",
    "print(\"Classifier 3\")\n",
    "a = np.array([clf3.fit(X[train], y[train]).score(X[test], y[test]) for train, test in k_fold.split(X)])\n",
    "print(a)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (a.mean(), a.std() * 2))\n",
    "\n",
    "b = np.array([clf3.fit(X[train], y[train]).score(X[test], y[test]) for train, test in strat_k_fold.split(X,y)])\n",
    "print(b)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (b.mean(), b.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.16)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf2, X, y, cv=k_fold, n_jobs=-1) #, scoring='precision_macro')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "Accuracy: 0.92 (+/- 0.53)\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut() # this sucks! what the STD DEV\n",
    "c = [clf2.fit(X[train], y[train]).score(X[test], y[test]) for train, test in loo.split(X,y)]\n",
    "print(c)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.array(c).mean(), np.array(c).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
