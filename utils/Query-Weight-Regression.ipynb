{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "%store -r resultDict\n",
    "%store -r qidInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 (210314, 0.929204) (111111, 0.902655)\n",
      "103 (100111, 0.568966) (111111, 0.568966)\n",
      "171 (111111, 0.439024) (111111, 0.439024)\n",
      "15 (521313, 0.709497) (111111, 0.653631)\n",
      "177 (111, 0.727273) (111111, 0.704545)\n",
      "18 (101111, 0.607143) (111111, 0.607143)\n",
      "112 (0, 0.0) (111111, 0.0)\n",
      "62 (101011, 0.650943) (111111, 0.632075)\n",
      "37 (10100, 1.0) (111111, 0.666667)\n",
      "198 (212003, 0.741379) (111111, 0.62069)\n",
      "134 (100001, 0.434783) (111111, 0.26087)\n",
      "68 (130202, 0.577778) (111111, 0.377778)\n",
      "163 (3005, 0.517857) (111111, 0.223214)\n",
      "193 (111001, 0.387097) (111111, 0.333333)\n",
      "138 (130000, 0.666667) (111111, 0.4)\n",
      "29 (101021, 0.571429) (111111, 0.428571)\n",
      "34 (211213, 0.421687) (111111, 0.313253)\n",
      "164 (213142, 0.358974) (111111, 0.320513)\n",
      "78 (100014, 0.208955) (111111, 0.089552)\n",
      "128 (101001, 0.216667) (111111, 0.2)\n",
      "8 (1003, 0.2) (111111, 0.066667)\n",
      "89 (21142, 0.634731) (111111, 0.60479)\n",
      "92 (1, 0.266667) (111111, 0.066667)\n",
      "61 (31122, 0.463415) (111111, 0.422764)\n",
      "197 (11113, 0.513274) (111111, 0.238938)\n",
      "195 (100002, 0.416667) (111111, 0.375)\n",
      "153 (122251, 0.606557) (111111, 0.590164)\n",
      "35 (11022, 0.936842) (111111, 0.915789)\n",
      "140 (1000, 0.25) (111111, 0.25)\n",
      "85 (110114, 0.194631) (111111, 0.147651)\n",
      "57 (422323, 0.347134) (111111, 0.334395)\n",
      "101 (110111, 0.27381) (111111, 0.261905)\n",
      "47 (500411, 0.963636) (111111, 0.909091)\n",
      "176 (1000, 0.117647) (111111, 0.058824)\n",
      "5 (101000, 0.875) (111111, 0.875)\n",
      "139 (25, 0.666667) (111111, 0.541667)\n",
      "31 (222313, 0.874172) (111111, 0.847682)\n",
      "65 (212225, 0.71875) (111111, 0.695312)\n",
      "99 (13, 0.460317) (111111, 0.047619)\n",
      "48 (110010, 0.833333) (111111, 0.833333)\n",
      "50 (100111, 0.875) (111111, 0.875)\n",
      "53 (301111, 0.336364) (111111, 0.290909)\n",
      "82 (102214, 0.462366) (111111, 0.365591)\n",
      "26 (222143, 0.597826) (111111, 0.543478)\n",
      "90 (100101, 0.553191) (111111, 0.553191)\n",
      "94 (102, 0.666667) (111111, 0.533333)\n",
      "187 (101021, 0.888889) (111111, 0.822222)\n",
      "51 (103112, 0.408163) (111111, 0.394558)\n",
      "137 (211111, 0.354497) (111111, 0.349206)\n",
      "69 (11112, 0.25) (111111, 0.194444)\n",
      "148 (1121, 0.4625) (111111, 0.4625)\n",
      "58 (321423, 0.652632) (111111, 0.621053)\n",
      "106 (0, 0.0) (111111, 0.0)\n",
      "175 (100111, 0.07438) (111111, 0.07438)\n",
      "91 (331103, 0.341772) (111111, 0.278481)\n",
      "143 (0, 0.0) (111111, 0.0)\n",
      "196 (201111, 0.54918) (111111, 0.52459)\n",
      "28 (112122, 0.913043) (111111, 0.84472)\n",
      "86 (10111, 0.579545) (111111, 0.579545)\n",
      "49 (112102, 0.769231) (111111, 0.666667)\n",
      "2 (211112, 0.890411) (111111, 0.863014)\n",
      "16 (413455, 0.95) (111111, 0.87)\n",
      "64 (200324, 0.352113) (111111, 0.288732)\n",
      "127 (101113, 0.677419) (111111, 0.645161)\n",
      "132 (1101, 0.2) (111111, 0.16)\n",
      "93 (1001, 0.457143) (111111, 0.457143)\n",
      "54 (111011, 0.562842) (111111, 0.546448)\n",
      "125 (101302, 0.404255) (111111, 0.319149)\n",
      "133 (312325, 0.35) (111111, 0.25)\n",
      "179 (101011, 0.727273) (111111, 0.636364)\n",
      "141 (1011, 0.617647) (111111, 0.617647)\n",
      "189 (100100, 0.333333) (111111, 0.333333)\n",
      "80 (511254, 0.335714) (111111, 0.307143)\n",
      "130 (111311, 0.567376) (111111, 0.556738)\n",
      "3 (101002, 0.525641) (111111, 0.525641)\n",
      "191 (1001, 0.5) (111111, 0.5)\n",
      "178 (211031, 0.566038) (111111, 0.54717)\n",
      "17 (120211, 0.612069) (111111, 0.594828)\n",
      "24 (31033, 0.576471) (111111, 0.458824)\n",
      "122 (10100, 0.4) (111111, 0.4)\n",
      "116 (111324, 0.381215) (111111, 0.342541)\n",
      "39 (123323, 0.901408) (111111, 0.84507)\n",
      "87 (200023, 0.344828) (111111, 0.155172)\n",
      "20 (0, 0.0) (111111, 0.0)\n",
      "158 (11021, 0.580153) (111111, 0.572519)\n",
      "46 (11101, 1.0) (111111, 1.0)\n",
      "126 (10001, 0.4) (111111, 0.0)\n",
      "108 (113123, 0.767123) (111111, 0.69863)\n",
      "23 (432004, 0.304) (111111, 0.136)\n",
      "167 (12, 0.724138) (111111, 0.189655)\n",
      "6 (101121, 0.971429) (111111, 0.885714)\n",
      "73 (310212, 0.452899) (111111, 0.423913)\n",
      "129 (100, 0.148148) (111111, 0.074074)\n",
      "102 (0, 0.0) (111111, 0.0)\n",
      "149 (20213, 0.629032) (111111, 0.564516)\n",
      "14 (250304, 0.673077) (111111, 0.567308)\n",
      "27 (244553, 0.809091) (111111, 0.727273)\n",
      "88 (11111, 0.373913) (111111, 0.373913)\n",
      "190 (100000, 0.3) (111111, 0.25)\n",
      "155 (411001, 0.626866) (111111, 0.462687)\n",
      "63 (220051, 0.616822) (111111, 0.598131)\n",
      "160 (1, 0.666667) (111111, 0.166667)\n",
      "194 (201, 0.914894) (111111, 0.914894)\n",
      "135 (100, 0.666667) (111111, 0.666667)\n",
      "124 (2101, 0.301587) (111111, 0.253968)\n",
      "71 (10102, 0.131868) (111111, 0.098901)\n",
      "36 (213305, 0.659574) (111111, 0.425532)\n",
      "168 (100100, 0.414634) (111111, 0.414634)\n",
      "12 (205241, 0.493827) (111111, 0.432099)\n",
      "174 (200213, 0.690909) (111111, 0.509091)\n",
      "156 (451233, 0.583333) (111111, 0.527778)\n",
      "119 (111111, 0.549296) (111111, 0.549296)\n",
      "97 (102304, 0.359375) (111111, 0.203125)\n",
      "19 (0, 0.0) (111111, 0.0)\n",
      "21 (12112, 0.90604) (111111, 0.865772)\n",
      "180 (404001, 0.169014) (111111, 0.084507)\n",
      "67 (101101, 0.440945) (111111, 0.433071)\n",
      "104 (11001, 0.238095) (111111, 0.190476)\n",
      "59 (100011, 0.229508) (111111, 0.180328)\n",
      "43 (100101, 0.969697) (111111, 0.909091)\n",
      "1 (300001, 0.918605) (111111, 0.860465)\n",
      "84 (311112, 0.495575) (111111, 0.477876)\n",
      "200 (11011, 0.769231) (111111, 0.730769)\n",
      "184 (111, 0.724638) (111111, 0.695652)\n",
      "186 (221123, 0.371287) (111111, 0.326733)\n",
      "146 (111002, 0.625) (111111, 0.5625)\n",
      "98 (202331, 0.362205) (111111, 0.283465)\n",
      "52 (101101, 0.787234) (111111, 0.787234)\n",
      "185 (13514, 0.481481) (111111, 0.345679)\n",
      "13 (200003, 0.7) (111111, 0.4)\n",
      "131 (101000, 0.75) (111111, 0.75)\n",
      "60 (311124, 0.559783) (111111, 0.521739)\n",
      "152 (0, 0.0) (111111, 0.0)\n",
      "162 (31, 0.40625) (111111, 0.375)\n",
      "120 (202, 0.48) (111111, 0.4)\n",
      "118 (21003, 0.208333) (111111, 0.208333)\n",
      "166 (112, 0.347826) (111111, 0.26087)\n",
      "42 (210022, 0.764706) (111111, 0.058824)\n",
      "157 (543224, 0.965517) (111111, 0.931034)\n",
      "154 (200211, 0.603448) (111111, 0.551724)\n",
      "161 (11, 0.428571) (111111, 0.428571)\n",
      "105 (210422, 0.40367) (111111, 0.33945)\n",
      "74 (111304, 0.5) (111111, 0.452381)\n",
      "199 (0, 0.108247) (111111, 0.108247)\n",
      "4 (21013, 1.0) (111111, 0.814815)\n",
      "7 (112011, 0.465753) (111111, 0.438356)\n",
      "182 (204155, 0.422925) (111111, 0.335968)\n",
      "165 (100004, 0.096774) (111111, 0.064516)\n",
      "72 (111, 0.076923) (111111, 0.038462)\n",
      "70 (0, 0.0) (111111, 0.0)\n",
      "83 (313, 0.40625) (111111, 0.09375)\n",
      "30 (432045, 0.754717) (111111, 0.716981)\n",
      "151 (433102, 0.290541) (111111, 0.182432)\n",
      "145 (211223, 0.386139) (111111, 0.366337)\n",
      "96 (122221, 0.623656) (111111, 0.569892)\n",
      "77 (101050, 0.397059) (111111, 0.367647)\n",
      "79 (514513, 0.696629) (111111, 0.640449)\n",
      "40 (0, 0.666667) (111111, 0.666667)\n",
      "142 (303, 0.733333) (111111, 0.466667)\n",
      "45 (511425, 0.699115) (111111, 0.637168)\n",
      "111 (112225, 0.546099) (111111, 0.503546)\n",
      "38 (303134, 0.537415) (111111, 0.435374)\n",
      "172 (310111, 0.563218) (111111, 0.534483)\n",
      "150 (100000, 0.25) (111111, 0.25)\n",
      "114 (211, 0.269231) (111111, 0.269231)\n",
      "76 (121212, 0.247191) (111111, 0.219101)\n",
      "144 (101111, 0.381443) (111111, 0.360825)\n",
      "170 (2032, 0.375) (111111, 0.25)\n",
      "55 (1533, 0.690722) (111111, 0.618557)\n",
      "136 (200054, 0.242424) (111111, 0.030303)\n",
      "66 (300011, 0.269231) (111111, 0.192308)\n",
      "32 (324222, 0.368794) (111111, 0.326241)\n",
      "10 (553040, 0.73) (111111, 0.51)\n",
      "115 (1000, 0.5) (111111, 0.5)\n",
      "181 (111, 0.633333) (111111, 0.3)\n",
      "173 (200112, 0.578125) (111111, 0.5625)\n",
      "169 (100012, 0.621359) (111111, 0.262136)\n",
      "9 (111201, 0.576471) (111111, 0.541176)\n",
      "11 (200111, 0.876923) (111111, 0.815385)\n",
      "123 (101111, 0.438776) (111111, 0.438776)\n",
      "192 (510005, 0.390476) (111111, 0.314286)\n",
      "44 (204111, 0.390625) (111111, 0.328125)\n",
      "75 (352142, 0.718391) (111111, 0.66092)\n",
      "109 (100110, 0.272727) (111111, 0.272727)\n",
      "107 (20112, 0.730769) (111111, 0.673077)\n",
      "159 (1011, 0.548387) (111111, 0.548387)\n",
      "41 (212255, 0.779412) (111111, 0.632353)\n",
      "117 (100101, 0.384615) (111111, 0.358974)\n",
      "121 (101111, 0.230769) (111111, 0.230769)\n",
      "56 (1201, 0.361702) (111111, 0.361702)\n",
      "22 (401122, 0.831858) (111111, 0.725664)\n",
      "33 (211121, 0.921986) (111111, 0.893617)\n",
      "110 (111020, 0.603175) (111111, 0.444444)\n",
      "81 (14, 0.571429) (111111, 0.387755)\n",
      "113 (0, 0.125) (111111, 0.125)\n",
      "183 (100, 0.193548) (111111, 0.16129)\n",
      "147 (211122, 0.536723) (111111, 0.451977)\n",
      "188 (425, 0.466667) (111111, 0.333333)\n",
      "0.511675444444\n",
      "0.436620954545\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "%store -r query2classic\n",
    "%store -r query2best\n",
    "\n",
    "for query, best in query2best.items():\n",
    "    print(query, best, query2classic[query])\n",
    "    \n",
    "    \n",
    "best_val = [y for x,y in query2best.values()]\n",
    "classic_val= [y for x,y in query2classic.values()]\n",
    "\n",
    "print(np.mean(best_val))\n",
    "print(np.mean(classic_val))\n",
    "print(len(query2best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'faceted', 'french lick resort and casino')\n",
      "(44, 'ambiguous', 'map of the united states')\n",
      "(54, 'faceted', 'president of the united states')\n",
      "(59, 'faceted', 'how to build a fence')\n",
      "(70, 'faceted', 'to be or not to be that is the question')\n",
      "(101, 'faceted', 'ritz carlton lake las vegas')\n",
      "(118, 'faceted', 'poem in your pocket day')\n",
      "(133, 'faceted', 'all men are created equal')\n",
      "(149, 'faceted', 'uplift at yellowstone national park')\n",
      "(169, 'faceted', 'battles in the civil war')\n",
      "(177, 'faceted', 'best long term care insurance')\n",
      "10\n",
      "(99, 'ambiguous', 'satellite')\n"
     ]
    }
   ],
   "source": [
    "q_len = []\n",
    "for q in qidInfo.values():\n",
    "    if len(q[2].split(\" \")) >4:\n",
    "        print(q)\n",
    "    q_len.append(len(q[2].split(\" \")))\n",
    "print(max(q_len))\n",
    "\n",
    "\n",
    "print(qidInfo[99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    data = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "    qid = int(data[0])\n",
    "    df_list = json.loads(data[2])\n",
    "    docid = int(data[4])\n",
    "    docno = data[5]\n",
    "    doclen = int(data[6])\n",
    "    tf_list = np.array(json.loads(data[7]))\n",
    "    return df_list, tf_list, doclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTermFeatures(qid_str):\n",
    "    directory = \"/home/muntean/terrier-passage/tfs-per-qid/all-matches-top-10000/\"\n",
    "    filename = directory + \"all-matches-fields-tfs-qid-\" + qid_str + \"-filtered.txt\"\n",
    "    \n",
    "    countLines = 0\n",
    "    with open(filename, \"r\") as inputFile:    \n",
    "        df_list, sum_tf_list, sum_doclen = parseLine(inputFile.readline())\n",
    "        countLines += 1\n",
    "        #print(df_list, sum_tf_list, sum_doclen)\n",
    "        for line in inputFile:\n",
    "            countLines += 1\n",
    "            df_list, tf_list, doclen = parseLine(line)\n",
    "            #print(df_list, tf_list, doclen)\n",
    "            sum_tf_list = sum_tf_list + tf_list\n",
    "            sum_doclen = sum_doclen + doclen\n",
    "  \n",
    "        avg_tf_list =  sum_tf_list / countLines\n",
    "        avg_doclen = sum_doclen / countLines\n",
    "        \n",
    "    return df_list, np.sum(avg_tf_list, axis=0), avg_doclen, avg_tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2181380, 3281536, 4558633, 8101661, 10630912],\n",
       " array([  7.6783,   6.0622,  10.0557,   6.9605,   9.1605,  15.1481]),\n",
       " 949.8307,\n",
       " array([[ 4.0618,  2.9265,  5.0182,  3.4823,  4.406 ,  7.0163],\n",
       "        [ 1.1869,  0.9437,  1.5259,  1.0338,  1.3868,  2.1968],\n",
       "        [ 1.3799,  1.2612,  2.0393,  1.4368,  1.9702,  3.6685],\n",
       "        [ 0.9174,  0.7555,  1.181 ,  0.8117,  1.1203,  1.9064],\n",
       "        [ 0.1323,  0.1753,  0.2913,  0.1959,  0.2772,  0.3601]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTermFeatures(\"177\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf(N, df):\n",
    "    return math.log((N - df + 0.5) / (df + 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET them features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have 2 query types: faceted, ambiguous\n",
    "\"\"\"\n",
    "N = 50220423\n",
    "AVG_LEN = 963.90334\n",
    "\n",
    "def getQueryFeatures(info):\n",
    "    featureDict=defaultdict(float)\n",
    "    \n",
    "    # Query Type\n",
    "    if info[1] == \"faceted\":\n",
    "        featureDict[\"faceted\"]=1\n",
    "        featureDict[\"ambiguous\"]=0\n",
    "    else:\n",
    "        featureDict[\"faceted\"]=0\n",
    "        featureDict[\"ambiguous\"]=1\n",
    "        \n",
    "    # Query len\n",
    "    featureDict[\"qlen\"] = len(info[2].split(\" \"))\n",
    "    \n",
    "    # Stopwords\n",
    "    featureDict[\"num_stopwords\"] = len([x for x in info[2].split(\" \") if x in ENGLISH_STOP_WORDS])\n",
    "    \n",
    "    # Term info\n",
    "    df_list, tf_list_avg, doclen_avg, tf_list_per_term  = getTermFeatures(str(qid))\n",
    "    \n",
    "    featureDict[\"deltaQlenDF\"] = len(info[2].split(\" \")) - len(df_list) # should be #stopwords but ...\n",
    "    \n",
    "#     dfs = np.zeros(5)  # max 5 terms so we 0 fill the vector\n",
    "#     for j in range(len(df_list)):\n",
    "#         dfs[j]=df_list[j]\n",
    "#     for i, df in enumerate(dfs):\n",
    "#         featureDict[\"DF\"+str(i+1)] = df\n",
    "           \n",
    "#     for i, df in enumerate(dfs):\n",
    "#         featureDict[\"IDF\"+str(i+1)] = idf(N, df)\n",
    "        \n",
    "#     term_tf_list = np.zeros(5*6) # max 5 terms each with a list of 6 TFs_avg\n",
    "#     for i, tf_list in enumerate(tf_list_per_term):\n",
    "#         for j, value in enumerate(tf_list):\n",
    "#             term_tf_list[(i*6)+j] = value\n",
    "#     for i, term_tf in enumerate(term_tf_list):\n",
    "#         featureDict[\"T\"+str(i//6+1)+\"TF\" + str(i%6 + 1)] = term_tf\n",
    "    \n",
    "    \n",
    "    featureDict[\"maxDF\"] = max(df_list)\n",
    "    featureDict[\"minDF\"] = min(df_list)\n",
    "    featureDict[\"avgDF\"] = np.mean(df_list)\n",
    "    \n",
    "#     featureDict[\"maxIDF\"] = max([idf(N, df) for df in df_list])\n",
    "#     featureDict[\"minIDF\"] = min([idf(N, df) for df in df_list])\n",
    "#     featureDict[\"avgIDF\"] = np.mean([idf(N, df) for df in df_list])\n",
    "    \n",
    "#     for i,tf_avg in enumerate(tf_list_avg):\n",
    "#         featureDict[\"avgTF\"+str(i+1)] = tf_avg\n",
    "#     featureDict[\"avgDoclen\"] = doclen_avg\n",
    "    \n",
    "#     deltaTF = [t - s for s, t in zip(tf_list_avg, tf_list_avg[1:])]\n",
    "#     for i, delta in enumerate(deltaTF):\n",
    "#         featureDict[\"deltaTF\"+str(i+1)] = delta\n",
    "\n",
    "    return featureDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_as_list = list()\n",
    "y = list()\n",
    "for qid, best in query2best.items():\n",
    "    featureDict = getQueryFeatures(qidInfo[qid])\n",
    "    X_as_list.append(list(featureDict.values()))\n",
    "    fieldWeight = str(best[0]).zfill(6)\n",
    "    y.append(int(fieldWeight[0]))\n",
    "\n",
    "feature_name = list(featureDict.keys())\n",
    "\n",
    "# X is the matrix\n",
    "# y are the classes\n",
    "X = np.array(X_as_list)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   4.26870000e+04\n",
      "    4.26870000e+04   4.26870000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   3.00000000e+00 ...,   4.86093000e+05\n",
      "    4.79420000e+04   2.11310667e+05]\n",
      " [  1.00000000e+00   0.00000000e+00   2.00000000e+00 ...,   7.17545000e+05\n",
      "    6.43767000e+05   6.80656000e+05]\n",
      " ..., \n",
      " [  1.00000000e+00   0.00000000e+00   3.00000000e+00 ...,   8.14602200e+06\n",
      "    1.03621600e+06   3.42953200e+06]\n",
      " [  1.00000000e+00   0.00000000e+00   4.00000000e+00 ...,   1.13052770e+07\n",
      "    1.02399000e+05   5.46757425e+06]\n",
      " [  1.00000000e+00   0.00000000e+00   3.00000000e+00 ...,   1.06634160e+07\n",
      "    5.15455500e+06   7.65860067e+06]]\n",
      "[2 1 1 5 0 1 0 1 0 2 1 1 0 1 1 1 2 2 1 1 0 0 0 0 0 1 1 0 0 1 4 1 5 0 1 0 2\n",
      " 2 0 1 1 3 1 2 1 0 1 1 2 0 0 3 0 1 3 0 2 1 0 1 2 4 2 1 0 0 1 1 3 1 0 1 5 1\n",
      " 1 0 2 1 0 0 1 1 2 0 0 0 0 1 4 0 1 3 0 0 0 2 2 0 1 4 2 0 0 0 0 0 2 1 2 2 4\n",
      " 1 1 0 0 4 1 0 1 1 3 3 0 0 2 1 2 1 0 2 1 3 0 0 0 0 0 2 5 2 0 2 1 0 0 1 2 1\n",
      " 0 0 0 4 4 2 1 1 5 0 0 5 1 3 3 1 0 1 1 0 0 2 3 3 5 0 0 2 1 1 2 1 5 2 3 1 0\n",
      " 0 2 1 1 0 4 2 1 0 0 0 2 0]\n",
      "198\n",
      "8\n",
      "['faceted', 'ambiguous', 'qlen', 'num_stopwords', 'deltaQlenDF', 'maxDF', 'minDF', 'avgDF']\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(len(feature_name))\n",
    "print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler() # [0,1]\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler() # [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "regressor1 = DecisionTreeRegressor(random_state=0, max_depth=2)\n",
    "regressor2 = DecisionTreeRegressor(random_state=0, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 5 0 1 0 1 0 2 1 1 0 1 1 1 2 2 1 1 0 0 0 0 0 1 1 0 0 1 4 1 5 0 1 0 2\n",
      " 2 0 1]\n",
      "[ 2.3125      1.33018868  2.3125      1.33018868  1.33018868  1.33018868\n",
      "  0.6         1.33018868  0.6         1.33018868  0.6         1.33018868\n",
      "  1.33018868  1.33018868  1.33018868  1.33018868  1.33018868  1.33018868\n",
      "  2.3125      0.6         2.3125      0.6         1.33018868  1.33018868\n",
      "  1.33018868  1.33018868  0.6         0.6         1.33018868  2.3125\n",
      "  1.33018868  1.33018868  0.6         1.33018868  1.33018868  1.33018868\n",
      "  1.33018868  1.33018868  1.33018868  1.33018868]\n",
      "[2 1 2 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 2 0 2 0 1 1 1 1 0 0 1 2 1 1 0 1 1 1 1\n",
      " 1 1 1]\n",
      "[ 2.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  2.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.]\n",
      "MSE round:  1.65\n",
      "Accuracy round:  0.375\n",
      "MSE :  1.85\n",
      "Accuracy :  0.4\n",
      "\n",
      "[1 3 1 2 1 0 1 1 2 0 0 3 0 1 3 0 2 1 0 1 2 4 2 1 0 0 1 1 3 1 0 1 5 1 1 0 2\n",
      " 1 0 0]\n",
      "[ 1.34108527  1.34108527  1.34108527  1.34108527  0.59259259  1.34108527\n",
      "  1.34108527  0.59259259  1.34108527  1.34108527  0.59259259  1.34108527\n",
      "  1.34108527  0.59259259  1.34108527  0.59259259  1.34108527  1.34108527\n",
      "  1.34108527  1.34108527  1.34108527  1.34108527  0.59259259  0.59259259\n",
      "  1.34108527  0.59259259  1.34108527  0.59259259  1.34108527  1.34108527\n",
      "  1.34108527  1.34108527  1.34108527  0.59259259  1.34108527  1.34108527\n",
      "  0.59259259  1.34108527  1.34108527  0.59259259]\n",
      "[1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 1 0]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.]\n",
      "MSE round:  1.475\n",
      "Accuracy round:  0.4\n",
      "MSE :  1.675\n",
      "Accuracy :  0.35\n",
      "\n",
      "[1 1 2 0 0 0 0 1 4 0 1 3 0 0 0 2 2 0 1 4 2 0 0 0 0 0 2 1 2 2 4 1 1 0 0 4 1\n",
      " 0 1 1]\n",
      "[ 1.38095238  1.38095238  1.38095238  1.38095238  1.38095238  0.63333333\n",
      "  1.38095238  1.38095238  1.38095238  1.38095238  4.          1.38095238\n",
      "  1.38095238  0.63333333  0.63333333  1.38095238  1.38095238  0.63333333\n",
      "  1.38095238  1.38095238  1.38095238  1.38095238  1.38095238  1.38095238\n",
      "  0.63333333  1.38095238  1.38095238  0.63333333  1.38095238  1.38095238\n",
      "  1.38095238  1.38095238  1.38095238  1.38095238  1.38095238  1.38095238\n",
      "  0.63333333  1.38095238  1.38095238  1.38095238]\n",
      "[1 1 1 1 1 0 1 1 1 1 4 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  4.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.]\n",
      "MSE round:  1.825\n",
      "Accuracy round:  0.25\n",
      "MSE :  1.75\n",
      "Accuracy :  0.325\n",
      "\n",
      "[3 3 0 0 2 1 2 1 0 2 1 3 0 0 0 0 0 2 5 2 0 2 1 0 0 1 2 1 0 0 0 4 4 2 1 1 5\n",
      " 0 0]\n",
      "[ 1.344       1.344       1.344       1.344       1.344       1.344       1.344\n",
      "  0.73076923  1.344       1.344       1.344       1.344       0.73076923\n",
      "  1.344       1.344       1.344       1.344       1.344       1.344       1.344\n",
      "  1.344       1.344       1.344       0.73076923  1.344       1.344       1.344\n",
      "  0.73076923  1.344       1.344       1.344       1.344       0.73076923\n",
      "  1.344       1.344       0.73076923  1.344       0.          1.344     ]\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1\n",
      " 0 1]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  0.  1.]\n",
      "MSE round:  2.17948717949\n",
      "Accuracy round:  0.230769230769\n",
      "MSE :  2.38461538462\n",
      "Accuracy :  0.205128205128\n",
      "\n",
      "[5 1 3 3 1 0 1 1 0 0 2 3 3 5 0 0 2 1 1 2 1 5 2 3 1 0 0 2 1 1 0 4 2 1 0 0 0\n",
      " 2 0]\n",
      "[ 0.69230769  1.28813559  0.69230769  1.28813559  1.28813559  1.28813559\n",
      "  1.28813559  1.28813559  0.69230769  1.28813559  1.28813559  1.28813559\n",
      "  0.69230769  1.28813559  0.69230769  0.69230769  1.28813559  0.69230769\n",
      "  0.69230769  0.69230769  1.28813559  0.69230769  0.69230769  1.28813559\n",
      "  0.69230769  1.28813559  1.28813559  1.28813559  0.69230769  1.28813559\n",
      "  0.69230769  1.28813559  1.28813559  0.69230769  0.69230769  1.28813559\n",
      "  1.28813559  1.28813559  1.28813559]\n",
      "[0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1\n",
      " 1 1]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.]\n",
      "MSE round:  2.46153846154\n",
      "Accuracy round:  0.282051282051\n",
      "MSE :  3.33333333333\n",
      "Accuracy :  0.282051282051\n",
      "\n",
      "AVG MSE:  2.19858974359\n",
      "AVG Accuracy:  0.312435897436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "mse_avg=[]\n",
    "acc_avg=[]\n",
    "for train, test in k_fold.split(X):\n",
    "    \n",
    "    max_abs_scaler.fit(X[train])  # Don't cheat - fit only on training data\n",
    "    xtrain_s = max_abs_scaler.transform(X[train])\n",
    "    xtest_s = max_abs_scaler.transform(X[test])  # apply same transformation to test data\n",
    "    \n",
    "    regressor1.fit(xtrain_s, y[train])\n",
    "    y_pred = regressor1.predict(xtest_s)\n",
    "    y_pred_floor = y_pred.astype(int)\n",
    "    y_pred_round = np.around(y_pred)\n",
    "    print(y[test])\n",
    "    print(y_pred)\n",
    "    print(y_pred_floor)\n",
    "    print(y_pred_round)\n",
    "    print(\"MSE round: \", mean_squared_error(y[test], y_pred_round))\n",
    "    print(\"Accuracy round: \", sum([x==y for (x,y) in zip(y[test], y_pred_round)])/len(y_pred))\n",
    "    print(\"MSE : \",mean_squared_error(y[test], y_pred_floor))\n",
    "    mse_avg.append(mean_squared_error(y[test], y_pred_floor))\n",
    "    print(\"Accuracy : \", sum([x==y for (x,y) in zip(y[test], y_pred_floor)])/len(y_pred))\n",
    "    acc_avg.append(sum([x==y for (x,y) in zip(y[test], y_pred_floor)])/len(y_pred))\n",
    "    print()\n",
    "    \n",
    "print(\"AVG MSE: \", np.mean(mse_avg))\n",
    "print(\"AVG Accuracy: \", np.mean(acc_avg))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 5 0 1 0 1 0 2 1 1 0 1 1 1 2 2 1 1 0 0 0 0 0 1 1 0 0 1 4 1 5 0 1 0 2\n",
      " 2 0 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[ 1.29707661  1.15695515  1.26423829  1.36219072  1.14699484  1.17599891\n",
      "  1.14499764  1.34095126  1.28143778  1.40516753  1.21050363  1.29604342\n",
      "  1.38602418  1.55127647  1.37257414  1.23420657  1.2966385   1.31155975\n",
      "  1.29675436  1.16243875  1.29681091  1.39437792  1.20828107  1.29544582\n",
      "  1.36234894  1.31661951  1.3945324   1.39432985  1.25258457  1.2124705\n",
      "  1.26182283  0.95701276  1.27503533  1.23543683  1.39091466  1.55935745\n",
      "  1.39355091  1.41904123  1.28664983  1.29136069]\n",
      "MSE :  1.55\n",
      "Accuracy :  0.4\n",
      "[1 3 1 2 1 0 1 1 2 0 0 3 0 1 3 0 2 1 0 1 2 4 2 1 0 0 1 1 3 1 0 1 5 1 1 0 2\n",
      " 1 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[ 1.15818469  1.20696874  1.32037871  1.09168528  1.32384138  1.32077966\n",
      "  1.31635627  1.29391781  1.11498027  1.29279272  1.01551173  1.32187563\n",
      "  1.03437766  1.08531753  1.4919866   1.23851905  1.26115379  1.32309048\n",
      "  1.17581621  1.19494352  0.9198485   1.04480825  1.41249641  1.194545\n",
      "  1.35183944  1.32332084  0.98524707  1.16276214  0.97190514  1.35405215\n",
      "  1.16906142  1.15897295  1.28280966  1.25613402  1.31593635  1.25763755\n",
      "  1.17652733  1.25834303  1.39698935  1.27004184]\n",
      "MSE :  1.7\n",
      "Accuracy :  0.375\n",
      "[1 1 2 0 0 0 0 1 4 0 1 3 0 0 0 2 2 0 1 4 2 0 0 0 0 0 2 1 2 2 4 1 1 0 0 4 1\n",
      " 0 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[ 1.32320298  1.32096403  1.63987963  1.29010204  1.59393046  1.25159241\n",
      "  1.44970137  1.21535931  1.24383647  1.28505215  1.29286017  1.32702503\n",
      "  1.28861442  1.24253003  1.22578217  1.30585294  1.30595396  1.28639751\n",
      "  1.21528581  1.47057207  1.29187395  1.29208807  1.25105722  1.32552021\n",
      "  1.32613328  1.24163499  1.29878685  1.30791398  1.30566945  1.26219789\n",
      "  1.34755884  1.23396596  1.29673958  1.08198645  1.30563702  1.29581025\n",
      "  1.30523208  1.29594617  1.46654793  1.23191224]\n",
      "MSE :  1.6\n",
      "Accuracy :  0.275\n",
      "[3 3 0 0 2 1 2 1 0 2 1 3 0 0 0 0 0 2 5 2 0 2 1 0 0 1 2 1 0 0 0 4 4 2 1 1 5\n",
      " 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "[ 1.24941516  1.15575502  1.09654542  1.24486895  1.27946399  1.15807347\n",
      "  1.27976465  1.20444255  1.23942544  1.20685155  1.07918307  1.20446569\n",
      "  1.15377054  1.20444821  1.29020118  1.52664151  1.26427949  1.37290115\n",
      "  1.25447884  1.27945527  1.32802396  1.19039218  1.20445674  1.2794442\n",
      "  1.27951205  1.36800889  1.16092846  1.11533281  1.33374706  2.43785971\n",
      "  1.20483504  1.35416834  1.27944487  1.21326025  1.2046163   1.20444602\n",
      "  1.20446684  1.27944415  1.27106099]\n",
      "MSE :  2.28205128205\n",
      "Accuracy :  0.205128205128\n",
      "[5 1 3 3 1 0 1 1 0 0 2 3 3 5 0 0 2 1 1 2 1 5 2 3 1 0 0 2 1 1 0 4 2 1 0 0 0\n",
      " 2 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1]\n",
      "[ 1.17812852  1.10961201  1.09181985  1.22070549  1.0690257   1.05477056\n",
      "  1.17482316  1.11516276  1.25415334  1.23740174  1.14223725  1.15395799\n",
      "  1.21816647  1.28964655  1.06352669  1.31058419  1.19713449  0.94413658\n",
      "  1.12649776  1.09191807  1.09530432  1.08646506  0.76882136  1.30041197\n",
      "  0.9608805   1.11872668  1.30106765  1.0773632   1.0934055   1.00598357\n",
      "  0.96829294  1.17299902  1.18681348  1.20757465  1.24402603  1.09360284\n",
      "  1.16591912  1.1884892   1.08559023]\n",
      "MSE :  2.5641025641\n",
      "Accuracy :  0.25641025641\n",
      "\n",
      "AVG MSE:  1.93923076923\n",
      "AVG Accuracy:  0.302307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muntean/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/muntean/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/muntean/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/muntean/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/muntean/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X)\n",
    "sgdreg = SGDRegressor(penalty='l2', alpha=0.15, n_iter=200, random_state=5)\n",
    "\n",
    "mse_avg=[]\n",
    "acc_avg=[]\n",
    "for train, test in k_fold.split(X):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[train])  # Don't cheat - fit only on training data\n",
    "    xtrain_s = scaler.transform(X[train])\n",
    "    xtest_s = scaler.transform(X[test])  # apply same transformation to test data\n",
    "    sgdreg.fit(xtrain_s,y[train])\n",
    "    y_pred = sgdreg.predict(xtest_s).astype(int)\n",
    "    print(y[test])\n",
    "    print(y_pred)\n",
    "    print(sgdreg.predict(xtest_s))\n",
    "    print(\"MSE : \", mean_squared_error(y[test], y_pred))\n",
    "    mse_avg.append(mean_squared_error(y[test], y_pred))\n",
    "    print(\"Accuracy : \", sum([x==y for (x,y) in zip(y[test], y_pred)])/len(y_pred))\n",
    "    acc_avg.append(sum([x==y for (x,y) in zip(y[test], y_pred)])/len(y_pred))\n",
    "print()\n",
    "print(\"AVG MSE: \", np.mean(mse_avg))\n",
    "print(\"AVG Accuracy: \", np.mean(acc_avg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 5 0 1 0 1 0 2 1 1 0 1 1 1 2 2 1 1 0 0 0 0 0 1 1 0 0 1 4 1 5 0 1 0 2\n",
      " 2 0 1]\n",
      "[ 1.81004271  1.54570222  0.77956171  1.16474611  1.10067929  0.64642318\n",
      "  0.64266835  0.40875506  0.57046098  0.88992458  0.53888626  1.60137233\n",
      "  0.75427298  2.72878582  0.41888383  1.53299837  0.77011574  1.86142833\n",
      "  1.60408665  0.6299007   1.60408665  0.56719069  1.08373668  0.7139055\n",
      "  0.67752099  1.24845118  0.67486576  0.71353429  1.82102998  2.09552515\n",
      "  1.54063256  1.37077095  0.95213756  0.70278094  2.88471194  2.18323179\n",
      "  1.1106651   1.58774636  0.73021718  1.21531581]\n",
      "[1 1 0 1 1 0 0 0 0 0 0 1 0 2 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 2 1 1 0 0 2 2 1\n",
      " 1 0 1]\n",
      "[ 2.  2.  1.  1.  1.  1.  1.  0.  1.  1.  1.  2.  1.  3.  0.  2.  1.  2.\n",
      "  2.  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  1.  1.  1.  3.  2.\n",
      "  1.  2.  1.  1.]\n",
      "MSE round:  1.925\n",
      "Accuracy round:  0.275\n",
      "MSE :  2.0\n",
      "Accuracy :  0.4\n",
      "\n",
      "[1 3 1 2 1 0 1 1 2 0 0 3 0 1 3 0 2 1 0 1 2 4 2 1 0 0 1 1 3 1 0 1 5 1 1 0 2\n",
      " 1 0 0]\n",
      "[ 0.88396451  1.0813034   0.64430569  1.02588771  0.95489004  1.52404856\n",
      "  0.65651078  0.97465692  1.18751641  1.4742096   0.84273886  0.95811311\n",
      "  0.51448276  0.69310182  0.54162748  0.36616851  1.37929407  0.69174051\n",
      "  1.45143238  1.43178274  1.78804764  0.721526    0.32129588  0.61768003\n",
      "  1.24283693  0.92414677  1.71006711  0.54036399  0.79814663  0.87221697\n",
      "  1.15599042  0.95147498  2.41157482  0.37902397  1.66977613  1.37929407\n",
      "  1.06020733  1.14030822  0.69195764  0.35670275]\n",
      "[0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 2 0 1 1 1\n",
      " 1 0 0]\n",
      "[ 1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.\n",
      "  1.  1.  2.  1.  0.  1.  1.  1.  2.  1.  1.  1.  1.  1.  2.  0.  2.  1.\n",
      "  1.  1.  1.  0.]\n",
      "MSE round:  1.45\n",
      "Accuracy round:  0.4\n",
      "MSE :  2.075\n",
      "Accuracy :  0.25\n",
      "\n",
      "[1 1 2 0 0 0 0 1 4 0 1 3 0 0 0 2 2 0 1 4 2 0 0 0 0 0 2 1 2 2 4 1 1 0 0 4 1\n",
      " 0 1 1]\n",
      "[ 0.76705334  1.18759384  1.95054623  0.66559822  0.77844326  0.36669204\n",
      "  0.61509964  1.0890612   1.26618458  2.72195043  0.78192653  1.33948275\n",
      "  0.88480296  0.77776838  1.20193577  1.05865372  1.66462493  0.66271071\n",
      "  0.88492735  1.6538789   1.34453061  0.88779369  1.36856868  2.57180759\n",
      "  0.85215284  0.44223396  1.33029415  3.18710254  0.65379592  2.04276756\n",
      "  1.01553681  0.44586893  1.6187215   1.45794105  0.65379592  1.05949093\n",
      "  0.48525468  1.104401    1.70801128  0.44586893]\n",
      "[0 1 1 0 0 0 0 1 1 2 0 1 0 0 1 1 1 0 0 1 1 0 1 2 0 0 1 3 0 2 1 0 1 1 0 1 0\n",
      " 1 1 0]\n",
      "[ 1.  1.  2.  1.  1.  0.  1.  1.  1.  3.  1.  1.  1.  1.  1.  1.  2.  1.\n",
      "  1.  2.  1.  1.  1.  3.  1.  0.  1.  3.  1.  2.  1.  0.  2.  1.  1.  1.\n",
      "  0.  1.  2.  0.]\n",
      "MSE round:  1.975\n",
      "Accuracy round:  0.25\n",
      "MSE :  1.775\n",
      "Accuracy :  0.4\n",
      "\n",
      "[3 3 0 0 2 1 2 1 0 2 1 3 0 0 0 0 0 2 5 2 0 2 1 0 0 1 2 1 0 0 0 4 4 2 1 1 5\n",
      " 0 0]\n",
      "[ 0.5670162   2.31441064  0.6284354   2.98411695  1.87128731  1.11996075\n",
      "  1.7879488   0.91354761  1.98681526  1.11792757  2.81434626  0.96987711\n",
      "  1.10547709  1.15154989  2.10628544  1.13511384  4.48378649  2.08931121\n",
      "  1.34087948  2.01011855  1.80335461  0.86881169  1.47100735  0.80960061\n",
      "  1.20606379  0.76759697  1.651485    0.24403512  2.6493323   1.24847366\n",
      "  0.97624218  2.14206825  0.95685505  1.32491589  0.15146645  1.21218419\n",
      "  0.9330703   0.80960061  2.59529932]\n",
      "[0 2 0 2 1 1 1 0 1 1 2 0 1 1 2 1 4 2 1 2 1 0 1 0 1 0 1 0 2 1 0 2 0 1 0 1 0\n",
      " 0 2]\n",
      "[ 1.  2.  1.  3.  2.  1.  2.  1.  2.  1.  3.  1.  1.  1.  2.  1.  4.  2.\n",
      "  1.  2.  2.  1.  1.  1.  1.  1.  2.  0.  3.  1.  1.  2.  1.  1.  0.  1.\n",
      "  1.  1.  3.]\n",
      "MSE round:  3.25641025641\n",
      "Accuracy round:  0.25641025641\n",
      "MSE :  3.41025641026\n",
      "Accuracy :  0.230769230769\n",
      "\n",
      "[5 1 3 3 1 0 1 1 0 0 2 3 3 5 0 0 2 1 1 2 1 5 2 3 1 0 0 2 1 1 0 4 2 1 0 0 0\n",
      " 2 0]\n",
      "[ 1.46928423  1.32129879  1.41339426  1.53922073  0.93008145  1.73754898\n",
      "  0.67219814  2.78605798  2.53439051  1.16939915  0.9286034   1.35233361\n",
      "  0.33932773  0.63790882  0.64696489  2.20011071  1.15719223  1.95690526\n",
      "  1.12201626  0.73350946  0.6597985   1.14560482  0.80592138  1.50745586\n",
      "  1.18404253  1.14489118  1.05892472  0.37223574  1.38211303  2.57414205\n",
      "  0.66209025  1.79498799  1.65856361  1.61821457  0.6566634   0.6597985\n",
      "  1.50729451  2.20652813  0.90879335]\n",
      "[1 1 1 1 0 1 0 2 2 1 0 1 0 0 0 2 1 1 1 0 0 1 0 1 1 1 1 0 1 2 0 1 1 1 0 0 1\n",
      " 2 0]\n",
      "[ 1.  1.  1.  2.  1.  2.  1.  3.  3.  1.  1.  1.  0.  1.  1.  2.  1.  2.\n",
      "  1.  1.  1.  1.  1.  2.  1.  1.  1.  0.  1.  3.  1.  2.  2.  2.  1.  1.\n",
      "  2.  2.  1.]\n",
      "MSE round:  3.02564102564\n",
      "Accuracy round:  0.230769230769\n",
      "MSE :  3.25641025641\n",
      "Accuracy :  0.307692307692\n",
      "\n",
      "AVG MSE:  2.50333333333\n",
      "AVG Accuracy:  0.317692307692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = GradientBoostingRegressor(**params)\n",
    "\n",
    "mse_avg=[]\n",
    "acc_avg=[]\n",
    "for train, test in k_fold.split(X):\n",
    "    \n",
    "    max_abs_scaler.fit(X[train])  # Don't cheat - fit only on training data\n",
    "    xtrain_s = max_abs_scaler.transform(X[train])\n",
    "    xtest_s = max_abs_scaler.transform(X[test])  # apply same transformation to test data\n",
    "    \n",
    "    clf.fit(xtrain_s, y[train])\n",
    "    y_pred = clf.predict(xtest_s)\n",
    "    y_pred_floor = y_pred.astype(int)\n",
    "    y_pred_round = np.around(y_pred)\n",
    "    print(y[test])\n",
    "    print(y_pred)\n",
    "    print(y_pred_floor)\n",
    "    print(y_pred_round)\n",
    "    print(\"MSE round: \", mean_squared_error(y[test], y_pred_round))\n",
    "    print(\"Accuracy round: \", sum([x==y for (x,y) in zip(y[test], y_pred_round)])/len(y_pred))\n",
    "    print(\"MSE : \",mean_squared_error(y[test], y_pred_floor))\n",
    "    mse_avg.append(mean_squared_error(y[test], y_pred_floor))\n",
    "    print(\"Accuracy : \", sum([x==y for (x,y) in zip(y[test], y_pred_floor)])/len(y_pred))\n",
    "    acc_avg.append(sum([x==y for (x,y) in zip(y[test], y_pred_floor)])/len(y_pred))\n",
    "    print()\n",
    "    \n",
    "print(\"AVG MSE: \", np.mean(mse_avg))\n",
    "print(\"AVG Accuracy: \", np.mean(acc_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
